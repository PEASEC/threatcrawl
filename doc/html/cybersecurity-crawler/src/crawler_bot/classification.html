<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>cybersecurity-crawler.src.crawler_bot.classification API documentation</title>
<meta name="description" content="The module that handles all classification related tasks" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cybersecurity-crawler.src.crawler_bot.classification</code></h1>
</header>
<section id="section-intro">
<p>The module that handles all classification related tasks</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;The module that handles all classification related tasks
&#34;&#34;&#34;

from transformers import BertTokenizer, BertModel
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import torch
import trafilatura
from trafilatura.settings import use_config
from re import split
import json
from math import ceil

from src.crawler_bot.tools import unit_vector, angle_between, print_progress_bar
from src.crawler_bot.custom_logging import Logger

ML_MODEL = &#34;bert-base-uncased&#34;  # or &#34;CySecBERT&#34; or &#34;all-mpnet-base-v2&#34; (SentenceBERT)


class Classifier:
  &#34;&#34;&#34;Handles the classification of websites using CyBert

  Attributes:
    id_number: id of this specific instance of classifier
    name: name of this specific instance of classifier for logging
    logger: instance of the logging module
    model: the used model for classifying
    tokenizer: the used tokenizer
    myconfig: specific config for trafilatura
    ground_truth_vectors: the used ground_truth_vectors to compare against
    max_amount_of_sentences: max amount of used sentences of each document
&#34;&#34;&#34;

  def __init__(self, id_number: int, logger: Logger):
    &#34;&#34;&#34;Inits Classifier

    Args:
      id_number: the id of the classifier
      logger: instance of the custom logging module
    &#34;&#34;&#34;
    self.id_number = id_number
    self.name = &#34;Classifier#&#34; + str(self.id_number)
    self.logger = logger

    if ML_MODEL == &#34;CySecBERT&#34;:
      self.model = BertModel.from_pretrained(
          &#34;cybert/&#34;,
          output_hidden_states=True,
      )
      self.tokenizer = BertTokenizer.from_pretrained(&#34;./cybert/&#34;)

    elif ML_MODEL == &#34;bert-base-uncased&#34;:
      self.tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
      self.model = AutoModel.from_pretrained(&#34;bert-base-uncased&#34;,
                                             output_hidden_states=True)
    elif ML_MODEL == &#34;all-mpnet-base-v2&#34;:
      self.model = SentenceTransformer(
          &#34;sentence-transformers/all-mpnet-base-v2&#34;)

    self.myconfig = use_config(&#34;src/crawler_bot/custom_trafilatura_config.cfg&#34;)
    self.logger.log_debug(self.name, &#34;initialized&#34;)

  def pre_process_sentence(self, sentence) -&gt; tuple[torch.tensor, torch.tensor]:
    &#34;&#34;&#34;Preprocesses the given sentence to use it as input for a BERT classifier

    Args:
      sentence: string of a sentence to be prepared

    Returns:
      a tuple of tensors to be used as input for a BERT classifier
    &#34;&#34;&#34;
    # add start token and seperator token at the end
    marked_text = &#34;[CLS] &#34; + sentence + &#34; [SEP]&#34;
    # tokenize the text
    tokenized_text = self.tokenizer.tokenize(marked_text)

    # cut away everything over 512 tokens (while keeping last token)
    if len(tokenized_text) &gt; 512:
      tokenized_text = tokenized_text[0:511] + [tokenized_text[-1]]

    # transform into ids
    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)
    # create segment ids, we only have 1 sentence -&gt; only one vector with ones
    segments_ids = [1] * len(indexed_tokens)

    self.logger.log_debug(self.name,
                          &#34;amount of tokens: &#34; + str(len(segments_ids)))

    # create torch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensor = torch.tensor([segments_ids])

    return tokens_tensor, segments_tensor

  def create_token_vectors(self, tokens_tensor: torch.tensor,
                           segments_tensor: torch.tensor) -&gt; list[float]:
    &#34;&#34;&#34;Creates the token vectors for each token in a sentence

    Args:
      tokens_tensor: a tensor containing the tokens of a sentence
      segments_tensor: a tensor containing the segment tensor

    Returns:
      a list of vectors, one for each word of the input sentence
    &#34;&#34;&#34;
    # freeze model
    with torch.no_grad():
      outputs = self.model(tokens_tensor, segments_tensor)
      # first layer is just input layer -&gt; remove it
      hidden_states = outputs[2]  #[1:]

    # extract the embeddings by concatenating the last 4 layers
    token_embeddings = hidden_states[-1]
    token_embeddings = torch.cat([hidden_states[i] for i in [-1, -2, -3, -4]],
                                 dim=-1)
    # converting torchtensors to lists
    token_vectors = [token_embed.tolist() for token_embed in token_embeddings]

    return token_vectors[0]

  def get_sentence_vector(self, sentence) -&gt; list[float]:
    &#34;&#34;&#34;Creates a embedding vector for the input sentence

    Args:
      sentence: a string for which an embedding is needed

    Returns:
      a vector which is an embedding of the input sentence
    &#34;&#34;&#34;

    if ML_MODEL == &#34;all-mpnet-base-v2&#34;:
      return self.model.encode(sentence)

    tokens_tensor, segments_tensor = self.pre_process_sentence(sentence)
    token_vectors = self.create_token_vectors(tokens_tensor, segments_tensor)
    # create sentence vector from token vectors
    sentence_vector = []
    for index in range(len(token_vectors[0])):
      vector_element = 0
      for token_vector in token_vectors:
        vector_element += token_vector[index]
      sentence_vector.append(vector_element)

    return sentence_vector

  def get_text_vector(self,
                      html: str,
                      max_sentences: int = 0,
                      generate_sentence_gradients: bool = False,
                      get_most_important_sentence: bool = False) -&gt; dict:
    &#34;&#34;&#34;Creates an embedding vector for a whole document

    Args:
      html: the document for which an embedding is needed
      max_sentences: amount of sentences that should be considered, 0 = all
      generate_sentence_gradients: if True, a gradient of the document vector
        after every sentence is calculated and returned
      get_most_important_sentence: returns the most informative sentence
        (sentence with least difference to overall embedding)

    Returns:
      a dict with text_vector and the sentence_gradients list if requested
    &#34;&#34;&#34;
    # use trafilatura to extract main content
    main_content = trafilatura.extract(html, config=self.myconfig)

    if main_content is None:
      self.logger.log_warning(
          self.name, &#34;Trafilatura was not able to extract the main content&#34;)
      return None

    # split the main content into single sentences by splitting at
    # newline or sentence ending signs
    uncleaned_sentences = split(r&#34;\n|!|\.|\?&#34;, main_content)

    sentences = []

    # remove empty sentences and sentences that only consist of single words
    for sentence in uncleaned_sentences:
      if sentence.strip() == &#34;&#34;:
        continue
      if len(sentence.strip()) &lt;= 1:
        continue
      sentences.append(sentence.strip())

    self.logger.log_debug(self.name,
                          &#34;split into &#34; + str(len(sentences)) + &#34; sentences&#34;)

    if len(sentences) &lt; 1:
      return None

    # cut away unwanted sentences if too long
    if len(sentences) &gt; max_sentences &gt; 0:
      sentences = sentences[:max_sentences + 1]
      self.logger.log_debug(
          self.name, &#34;only &#34; + str(max_sentences) + &#34; sentences are used&#34;)

    sentence_vectors = []
    # generate a vector for every sentence
    i = 0
    for sentence in sentences:
      i += 1
      self.logger.log_debug(
          self.name, &#34;embedding sentence &#34; + str(i) + &#34;/&#34; + str(len(sentences)))
      sentence_vectors.append(self.get_sentence_vector(sentence))

    # create a whole vector by adding all sentence vectors
    text_vector = []
    # create 0 vector with correct length
    text_vector = [0 for _ in sentence_vectors[0]]

    if generate_sentence_gradients:
      sentence_gradients = []
      first = True
      # add up all sentences and create gradient after each step
      for sentence_vector in sentence_vectors:
        if first:
          text_vector = sentence_vector
          first = False
        else:
          old_text_vector = text_vector
          text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
          gradient = angle_between(unit_vector(old_text_vector),
                                   unit_vector(text_vector))
          sentence_gradients.append(gradient)

      result = {
          &#34;text_vector&#34;: text_vector,
          &#34;sentence_gradients&#34;: sentence_gradients
      }
    else:
      # just add up all sentence vectors
      for sentence_vector in sentence_vectors:
        text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
      result = {&#34;text_vector&#34;: text_vector}

    if get_most_important_sentence:
      index_min = len(sentence_vectors) - 1
      min_diff = 99999
      for index, sentence_vector in enumerate(sentence_vectors):
        current_diff = angle_between(unit_vector(sentence_vector),
                                     unit_vector(result[&#34;text_vector&#34;]))
        if current_diff &lt; min_diff:
          min_diff = current_diff
          index_min = index
      result[&#34;most_important_sentence&#34;] = sentences[index_min]

    return result

  def load_parameters_from_file(self, filename: str) -&gt; None:
    &#34;&#34;&#34;Loads the ground truth vectors and max_amount_of_sentences from the given
        file

    Args:
      filename: name of the file which holds the ground truth vectors

    Returns:
      None
    &#34;&#34;&#34;
    with open(filename, encoding=&#34;utf-8&#34;) as x:
      data = json.load(x)
    self.ground_truth_vectors = data[&#34;ground_truth_vectors&#34;]
    self.max_amount_of_sentences = data[&#34;parameters&#34;][&#34;max_amount_of_sentences&#34;]

  def set_parameters(self, ground_truth_vectors: dict,
                     max_amount_of_sentences: int) -&gt; None:
    &#34;&#34;&#34;Sets new ground truth vectors and the amount of sentences to be used for
        classification

    Args:
      ground_truth_vectors: a dict containing the new ground truth vectors
      max_sentences: defines the max amount of sentences

    Returns:
      None

    &#34;&#34;&#34;
    self.ground_truth_vectors = ground_truth_vectors
    self.max_amount_of_sentences = max_amount_of_sentences

  def is_relevant(self, url: str, html_document: str) -&gt; dict:
    &#34;&#34;&#34;Calculates the differences of the input document and decides if it is
        relevant or not

    Args:
      url: url of the html document
      html_document: the html document to be classified

    Returns:
      a dict containing &#34;relevant&#34; (bool), distances (to each category vector),
      relative distances (to each category vector) and guessed_category
    &#34;&#34;&#34;
    error_result = {
        &#34;relevant&#34;: False,
        &#34;distances&#34;: {},
        &#34;relative_distances&#34;: {},
        &#34;guessed_category&#34;: &#34;not_relevant&#34;
    }

    if not hasattr(self, &#34;ground_truth_vectors&#34;):
      self.logger.log_critical(self.name, &#34;Ground truth vectors not loaded&#34;)
      self.monitor.stop_everything(&#34;Ground truth vectors not loaded&#34;)
      return error_result
    if not hasattr(self, &#34;max_amount_of_sentences&#34;):
      self.logger.log_critical(self.name, &#34;Max amount of sentences not set&#34;)
      self.monitor.stop_everything(&#34;Max amount of sentences not set&#34;)
      return error_result

    if html_document == &#34;&#34; or url == &#34;&#34;:
      return error_result

    # get embedding
    embedding_result = self.get_text_vector(html_document,
                                            self.max_amount_of_sentences, False,
                                            False)

    if embedding_result is None:
      self.logger.log_error(self.name, &#34;cant get embedding for &#34; + url)
      return error_result

    # create unitvector
    embedding = unit_vector(embedding_result[&#34;text_vector&#34;])

    distances = {}
    relative_distances = {}
    relevant = False
    guessed_category = &#34;not_relevant&#34;
    smallest_distance = 99999

    # calculate distance and relative distance for every category
    for category, ground_truth in self.ground_truth_vectors.items():
      distance = angle_between(embedding, ground_truth[&#34;embedding&#34;])
      distances[category] = distance
      relative_distances[category] = distance / ground_truth[&#34;allowed_distance&#34;]
      # document is relevant, now check to which cat it has the lowest relative distance
      if distance &lt;= ground_truth[&#34;allowed_distance&#34;]:
        relevant = True
        if relative_distances[category] &lt; smallest_distance:
          smallest_distance = relative_distances[category]
          guessed_category = category

    result = {
        &#34;relevant&#34;: relevant,
        &#34;distances&#34;: distances,
        &#34;relative_distances&#34;: relative_distances,
        &#34;guessed_category&#34;: guessed_category
    }

    return result

  def calculate_ideal_amount_of_sentences(
      self, dataset: dict, ignore_categories: bool) -&gt; (int, dict):
    &#34;&#34;&#34;Calculates the ideal amount of sentences per document for classifiying
      them

    Args:
      dataset: the dataset as dictionary
      ignore_categories: decides if categories are used or only one vector is
                          created

    Returns:
      the calculated amount of sentences that should be used and the sentence
      gradients so they can be saved
    &#34;&#34;&#34;

    amount_documents = 0
    sentence_gradients = {}
    gradient_limit = 0.02
    counter = 1

    self.logger.log_info(self.name, &#34;calculating ideal amount of sentences&#34;)

    # get amount of all relevant documents
    for category, items in dataset.items():
      if category != &#34;not_relevant&#34;:
        amount_documents += len(items)

    # iterate through all relevant documents and retrieve the sentence gradients
    for category, items in dataset.items():
      if category == &#34;not_relevant&#34;:
        continue
      # if we ignore labels, every label other than not_relevant is relevant
      if ignore_categories:
        category = &#34;relevant&#34;

      sentence_gradients[category] = []
      # get embedding and gradients for each document with max. amount of
      # sentences
      for item in items:
        print_progress_bar(counter, amount_documents)
        embedding_result = self.get_text_vector(item[&#34;document&#34;], 0, True,
                                                False)
        sentence_gradients[category].append({
            &#34;url&#34;: item[&#34;url&#34;],
            &#34;sentence_gradients&#34;: embedding_result[&#34;sentence_gradients&#34;]
        })
        counter += 1

    # calculate the ideal amount of sentences by averaging the amount of
    # sentences where gradient &lt; gradient_limit
    indices_gradient_limit_reached = []
    for category, items in sentence_gradients.items():
      for item in items:
        # set searched index to last element
        index_gradient_limit_reached = len(item[&#34;sentence_gradients&#34;]) - 1
        # look for index where gradient gets below gradient limit
        for index, value in enumerate(item[&#34;sentence_gradients&#34;]):
          if value &lt;= gradient_limit:
            index_gradient_limit_reached = index
            break
        indices_gradient_limit_reached.append(index_gradient_limit_reached)

    # get ideal value by averaging the ideal amount of sentences of each
    # document
    ideal_amount_of_sentences = ceil(
        sum(indices_gradient_limit_reached) /
        len(indices_gradient_limit_reached))

    return ideal_amount_of_sentences, sentence_gradients

  def generate_ground_truth_vectors(
      self,
      dataset: dict,
      ignore_categories: bool = False,
      max_amount_of_sentences: int = 0,
      allowed_distance_average: bool = False,
      get_most_important_sentences: bool = False) -&gt; dict:
    &#34;&#34;&#34;Generates new ground truth vectors from a csv file
        category not_relevant will be ignored

    Args:
      dataset_filename: filename of the csv file
      ignore_categories: decides if categories are used or only one vector is
                          created
      max_amount_of_sentences: amount of sentences considered for
                                classification (0=all)
      allowed_distance_average: decides if allowed_distance is calculated as
        average distance of each datapoint from ground truth vector or as the
        maximum distance from all points (per category)
      get_most_important_sentences: additionally returns dict of most important
        sentence per document

    Returns:
      a dict containing the generated vectors and one containing the gradients
        for each step and optionally the most important sentences
    &#34;&#34;&#34;

    # generate embedding for every url (still grouped by category)
    ground_truth_vectors = {}
    single_embeddings = {}
    ground_truth_gradient = {}
    result = {}

    counter = 1
    amount_documents = 0

    # get amount of all relevant documents
    for category, items in dataset.items():
      if category != &#34;not_relevant&#34;:
        amount_documents += len(items)

    if get_most_important_sentences:
      most_important_sentences = {}

    # process all documents: generate embedding and add it to the category
    # embedding
    for category, items in dataset.items():
      if category == &#34;not_relevant&#34;:
        continue
      # if categories should be ignored, replace category with one big
      # one (ground_truth)
      if ignore_categories:
        category = &#34;ground_truth&#34;
      for item in items:
        print_progress_bar(counter, amount_documents)
        self.logger.log_debug(
            self.name, &#34;generating embedding &#34; + str(counter) + &#34;/&#34; +
            str(amount_documents) + &#34; (&#34; + item[&#34;url&#34;] + &#34;)&#34;)
        try:
          # get embedding
          embedding_result = self.get_text_vector(item[&#34;document&#34;],
                                                  max_amount_of_sentences,
                                                  False,
                                                  get_most_important_sentences)

          # add new vector to the existing one to generate category embedding
          if category in ground_truth_vectors:
            old_ground_truth = ground_truth_vectors[category]
            new_ground_truth = [
                sum(i)
                for i in zip(embedding_result[&#34;text_vector&#34;], old_ground_truth)
            ]
            ground_truth_vectors[category] = new_ground_truth

            # calculate the gradient to see how much the vector changed
            gradient = angle_between(unit_vector(old_ground_truth),
                                     unit_vector(new_ground_truth))
            if category in ground_truth_gradient:
              ground_truth_gradient[category].append(gradient)
            else:
              ground_truth_gradient[category] = [gradient]

          else:
            ground_truth_vectors[category] = embedding_result[&#34;text_vector&#34;]

          # create the unit vector and save the single document embedding to
          # calculate allowed_distance later
          embedding = unit_vector(embedding_result[&#34;text_vector&#34;])
          if category in single_embeddings:
            single_embeddings[category].append({
                &#34;url&#34;: item[&#34;url&#34;],
                &#34;embedding&#34;: embedding
            })
          else:
            single_embeddings[category] = [{
                &#34;url&#34;: item[&#34;url&#34;],
                &#34;embedding&#34;: embedding
            }]
          # add most important sentences to result if requested
          if get_most_important_sentences:
            self.logger.log_debug(self.name, &#34;adding most important sentences&#34;)
            if category in most_important_sentences:
              most_important_sentences[category].append({
                  &#34;url&#34;:
                      item[&#34;url&#34;],
                  &#34;most_important_sentence&#34;:
                      embedding_result[&#34;most_important_sentence&#34;]
              })
            else:
              most_important_sentences[category] = [{
                  &#34;url&#34;:
                      item[&#34;url&#34;],
                  &#34;most_important_sentence&#34;:
                      embedding_result[&#34;most_important_sentence&#34;]
              }]
        except Exception as e:
          self.logger.log_warning(
              self.name, &#34;problem with &#34; + item[&#34;url&#34;] + &#34;(&#34; + str(e) + &#34;)&#34;)

        counter += 1
    print(&#34;\n&#34;)

    if get_most_important_sentences:
      result[&#34;most_important_sentences&#34;] = most_important_sentences

    self.logger.log_debug(self.name, &#34;ground truth vectors generated&#34;)

    # normalize all vectors for easier processing
    for category, vector in ground_truth_vectors.items():
      ground_truth_vectors[category] = {
          &#34;embedding&#34;: unit_vector(vector).tolist()
      }

    self.logger.log_debug(self.name, &#34;ground truth vectors normalized&#34;)

    # calculate allowed distance for each url from the calculated ground truth
    # vectors per category
    allowed_distances = {}

    if allowed_distance_average:
      # allowed distance is calculated as average distance from ground truth
      for category, entries in single_embeddings.items():
        allowed_distances[category] = 0
        # add up all distances per category
        for entry in entries:
          distance = angle_between(entry[&#34;embedding&#34;],
                                   ground_truth_vectors[category][&#34;embedding&#34;])
          allowed_distances[category] += distance

        # devide by amount of documents
        if len(entries) &gt; 0:
          allowed_distances[category] = allowed_distances[category] / len(
              entries)
        else:
          allowed_distances[category] = 0
    else:
      # allowed distance is calculated as max distance from ground truth
      for category, entries in single_embeddings.items():
        allowed_distances[category] = 0
        for entry in entries:
          distance = angle_between(entry[&#34;embedding&#34;],
                                   ground_truth_vectors[category][&#34;embedding&#34;])
          # remember the greatest seen distance
          if distance &gt; allowed_distances[category]:
            allowed_distances[category] = distance

    # save in ground truth vectors
    for category, allowed_distance in allowed_distances.items():
      ground_truth_vectors[category][&#34;allowed_distance&#34;] = allowed_distance

    result[&#34;ground_truth_vectors&#34;] = ground_truth_vectors
    result[&#34;ground_truth_gradients&#34;] = ground_truth_gradient

    return result

  def classify_bulk(self, dataset: dict) -&gt; str:
    &#34;&#34;&#34;Classifies the given dataset and returns metrics

    Args:
      dataset: a dict of a dataset

    Returns:
      metrics

    &#34;&#34;&#34;
    result = {}
    self.logger.log_debug(self.name, &#34;starting to classify bulk&#34;)

    amount_documents = 0
    counter = 0

    for category, items in dataset.items():
      amount_documents += len(items)

    if amount_documents == 0:
      return {}

    # iterate through all items
    for category, items in dataset.items():
      for item in items:
        counter += 1
        self.logger.log_debug(self.name, &#34;classifiying &#34; + item[&#34;url&#34;])
        print_progress_bar(counter, amount_documents)

        # calculate distances for the document
        classification_result = self.is_relevant(item[&#34;url&#34;], item[&#34;document&#34;])

        if category in result:
          result[category].append({
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;classification_result&#34;: classification_result
          })
        else:
          result[category] = [{
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;classification_result&#34;: classification_result
          }]
    print(&#34;\n&#34;)
    return result</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier"><code class="flex name class">
<span>class <span class="ident">Classifier</span></span>
<span>(</span><span>id_number: int, logger: src.crawler_bot.custom_logging.Logger)</span>
</code></dt>
<dd>
<div class="desc"><p>Handles the classification of websites using CyBert</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>id_number</code></strong></dt>
<dd>id of this specific instance of classifier</dd>
<dt><strong><code>name</code></strong></dt>
<dd>name of this specific instance of classifier for logging</dd>
<dt><strong><code>logger</code></strong></dt>
<dd>instance of the logging module</dd>
<dt><strong><code>model</code></strong></dt>
<dd>the used model for classifying</dd>
<dt><strong><code>tokenizer</code></strong></dt>
<dd>the used tokenizer</dd>
<dt><strong><code>myconfig</code></strong></dt>
<dd>specific config for trafilatura</dd>
<dt><strong><code>ground_truth_vectors</code></strong></dt>
<dd>the used ground_truth_vectors to compare against</dd>
<dt><strong><code>max_amount_of_sentences</code></strong></dt>
<dd>max amount of used sentences of each document</dd>
</dl>
<p>Inits Classifier</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>id_number</code></strong></dt>
<dd>the id of the classifier</dd>
<dt><strong><code>logger</code></strong></dt>
<dd>instance of the custom logging module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Classifier:
  &#34;&#34;&#34;Handles the classification of websites using CyBert

  Attributes:
    id_number: id of this specific instance of classifier
    name: name of this specific instance of classifier for logging
    logger: instance of the logging module
    model: the used model for classifying
    tokenizer: the used tokenizer
    myconfig: specific config for trafilatura
    ground_truth_vectors: the used ground_truth_vectors to compare against
    max_amount_of_sentences: max amount of used sentences of each document
&#34;&#34;&#34;

  def __init__(self, id_number: int, logger: Logger):
    &#34;&#34;&#34;Inits Classifier

    Args:
      id_number: the id of the classifier
      logger: instance of the custom logging module
    &#34;&#34;&#34;
    self.id_number = id_number
    self.name = &#34;Classifier#&#34; + str(self.id_number)
    self.logger = logger

    if ML_MODEL == &#34;CySecBERT&#34;:
      self.model = BertModel.from_pretrained(
          &#34;cybert/&#34;,
          output_hidden_states=True,
      )
      self.tokenizer = BertTokenizer.from_pretrained(&#34;./cybert/&#34;)

    elif ML_MODEL == &#34;bert-base-uncased&#34;:
      self.tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
      self.model = AutoModel.from_pretrained(&#34;bert-base-uncased&#34;,
                                             output_hidden_states=True)
    elif ML_MODEL == &#34;all-mpnet-base-v2&#34;:
      self.model = SentenceTransformer(
          &#34;sentence-transformers/all-mpnet-base-v2&#34;)

    self.myconfig = use_config(&#34;src/crawler_bot/custom_trafilatura_config.cfg&#34;)
    self.logger.log_debug(self.name, &#34;initialized&#34;)

  def pre_process_sentence(self, sentence) -&gt; tuple[torch.tensor, torch.tensor]:
    &#34;&#34;&#34;Preprocesses the given sentence to use it as input for a BERT classifier

    Args:
      sentence: string of a sentence to be prepared

    Returns:
      a tuple of tensors to be used as input for a BERT classifier
    &#34;&#34;&#34;
    # add start token and seperator token at the end
    marked_text = &#34;[CLS] &#34; + sentence + &#34; [SEP]&#34;
    # tokenize the text
    tokenized_text = self.tokenizer.tokenize(marked_text)

    # cut away everything over 512 tokens (while keeping last token)
    if len(tokenized_text) &gt; 512:
      tokenized_text = tokenized_text[0:511] + [tokenized_text[-1]]

    # transform into ids
    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)
    # create segment ids, we only have 1 sentence -&gt; only one vector with ones
    segments_ids = [1] * len(indexed_tokens)

    self.logger.log_debug(self.name,
                          &#34;amount of tokens: &#34; + str(len(segments_ids)))

    # create torch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensor = torch.tensor([segments_ids])

    return tokens_tensor, segments_tensor

  def create_token_vectors(self, tokens_tensor: torch.tensor,
                           segments_tensor: torch.tensor) -&gt; list[float]:
    &#34;&#34;&#34;Creates the token vectors for each token in a sentence

    Args:
      tokens_tensor: a tensor containing the tokens of a sentence
      segments_tensor: a tensor containing the segment tensor

    Returns:
      a list of vectors, one for each word of the input sentence
    &#34;&#34;&#34;
    # freeze model
    with torch.no_grad():
      outputs = self.model(tokens_tensor, segments_tensor)
      # first layer is just input layer -&gt; remove it
      hidden_states = outputs[2]  #[1:]

    # extract the embeddings by concatenating the last 4 layers
    token_embeddings = hidden_states[-1]
    token_embeddings = torch.cat([hidden_states[i] for i in [-1, -2, -3, -4]],
                                 dim=-1)
    # converting torchtensors to lists
    token_vectors = [token_embed.tolist() for token_embed in token_embeddings]

    return token_vectors[0]

  def get_sentence_vector(self, sentence) -&gt; list[float]:
    &#34;&#34;&#34;Creates a embedding vector for the input sentence

    Args:
      sentence: a string for which an embedding is needed

    Returns:
      a vector which is an embedding of the input sentence
    &#34;&#34;&#34;

    if ML_MODEL == &#34;all-mpnet-base-v2&#34;:
      return self.model.encode(sentence)

    tokens_tensor, segments_tensor = self.pre_process_sentence(sentence)
    token_vectors = self.create_token_vectors(tokens_tensor, segments_tensor)
    # create sentence vector from token vectors
    sentence_vector = []
    for index in range(len(token_vectors[0])):
      vector_element = 0
      for token_vector in token_vectors:
        vector_element += token_vector[index]
      sentence_vector.append(vector_element)

    return sentence_vector

  def get_text_vector(self,
                      html: str,
                      max_sentences: int = 0,
                      generate_sentence_gradients: bool = False,
                      get_most_important_sentence: bool = False) -&gt; dict:
    &#34;&#34;&#34;Creates an embedding vector for a whole document

    Args:
      html: the document for which an embedding is needed
      max_sentences: amount of sentences that should be considered, 0 = all
      generate_sentence_gradients: if True, a gradient of the document vector
        after every sentence is calculated and returned
      get_most_important_sentence: returns the most informative sentence
        (sentence with least difference to overall embedding)

    Returns:
      a dict with text_vector and the sentence_gradients list if requested
    &#34;&#34;&#34;
    # use trafilatura to extract main content
    main_content = trafilatura.extract(html, config=self.myconfig)

    if main_content is None:
      self.logger.log_warning(
          self.name, &#34;Trafilatura was not able to extract the main content&#34;)
      return None

    # split the main content into single sentences by splitting at
    # newline or sentence ending signs
    uncleaned_sentences = split(r&#34;\n|!|\.|\?&#34;, main_content)

    sentences = []

    # remove empty sentences and sentences that only consist of single words
    for sentence in uncleaned_sentences:
      if sentence.strip() == &#34;&#34;:
        continue
      if len(sentence.strip()) &lt;= 1:
        continue
      sentences.append(sentence.strip())

    self.logger.log_debug(self.name,
                          &#34;split into &#34; + str(len(sentences)) + &#34; sentences&#34;)

    if len(sentences) &lt; 1:
      return None

    # cut away unwanted sentences if too long
    if len(sentences) &gt; max_sentences &gt; 0:
      sentences = sentences[:max_sentences + 1]
      self.logger.log_debug(
          self.name, &#34;only &#34; + str(max_sentences) + &#34; sentences are used&#34;)

    sentence_vectors = []
    # generate a vector for every sentence
    i = 0
    for sentence in sentences:
      i += 1
      self.logger.log_debug(
          self.name, &#34;embedding sentence &#34; + str(i) + &#34;/&#34; + str(len(sentences)))
      sentence_vectors.append(self.get_sentence_vector(sentence))

    # create a whole vector by adding all sentence vectors
    text_vector = []
    # create 0 vector with correct length
    text_vector = [0 for _ in sentence_vectors[0]]

    if generate_sentence_gradients:
      sentence_gradients = []
      first = True
      # add up all sentences and create gradient after each step
      for sentence_vector in sentence_vectors:
        if first:
          text_vector = sentence_vector
          first = False
        else:
          old_text_vector = text_vector
          text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
          gradient = angle_between(unit_vector(old_text_vector),
                                   unit_vector(text_vector))
          sentence_gradients.append(gradient)

      result = {
          &#34;text_vector&#34;: text_vector,
          &#34;sentence_gradients&#34;: sentence_gradients
      }
    else:
      # just add up all sentence vectors
      for sentence_vector in sentence_vectors:
        text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
      result = {&#34;text_vector&#34;: text_vector}

    if get_most_important_sentence:
      index_min = len(sentence_vectors) - 1
      min_diff = 99999
      for index, sentence_vector in enumerate(sentence_vectors):
        current_diff = angle_between(unit_vector(sentence_vector),
                                     unit_vector(result[&#34;text_vector&#34;]))
        if current_diff &lt; min_diff:
          min_diff = current_diff
          index_min = index
      result[&#34;most_important_sentence&#34;] = sentences[index_min]

    return result

  def load_parameters_from_file(self, filename: str) -&gt; None:
    &#34;&#34;&#34;Loads the ground truth vectors and max_amount_of_sentences from the given
        file

    Args:
      filename: name of the file which holds the ground truth vectors

    Returns:
      None
    &#34;&#34;&#34;
    with open(filename, encoding=&#34;utf-8&#34;) as x:
      data = json.load(x)
    self.ground_truth_vectors = data[&#34;ground_truth_vectors&#34;]
    self.max_amount_of_sentences = data[&#34;parameters&#34;][&#34;max_amount_of_sentences&#34;]

  def set_parameters(self, ground_truth_vectors: dict,
                     max_amount_of_sentences: int) -&gt; None:
    &#34;&#34;&#34;Sets new ground truth vectors and the amount of sentences to be used for
        classification

    Args:
      ground_truth_vectors: a dict containing the new ground truth vectors
      max_sentences: defines the max amount of sentences

    Returns:
      None

    &#34;&#34;&#34;
    self.ground_truth_vectors = ground_truth_vectors
    self.max_amount_of_sentences = max_amount_of_sentences

  def is_relevant(self, url: str, html_document: str) -&gt; dict:
    &#34;&#34;&#34;Calculates the differences of the input document and decides if it is
        relevant or not

    Args:
      url: url of the html document
      html_document: the html document to be classified

    Returns:
      a dict containing &#34;relevant&#34; (bool), distances (to each category vector),
      relative distances (to each category vector) and guessed_category
    &#34;&#34;&#34;
    error_result = {
        &#34;relevant&#34;: False,
        &#34;distances&#34;: {},
        &#34;relative_distances&#34;: {},
        &#34;guessed_category&#34;: &#34;not_relevant&#34;
    }

    if not hasattr(self, &#34;ground_truth_vectors&#34;):
      self.logger.log_critical(self.name, &#34;Ground truth vectors not loaded&#34;)
      self.monitor.stop_everything(&#34;Ground truth vectors not loaded&#34;)
      return error_result
    if not hasattr(self, &#34;max_amount_of_sentences&#34;):
      self.logger.log_critical(self.name, &#34;Max amount of sentences not set&#34;)
      self.monitor.stop_everything(&#34;Max amount of sentences not set&#34;)
      return error_result

    if html_document == &#34;&#34; or url == &#34;&#34;:
      return error_result

    # get embedding
    embedding_result = self.get_text_vector(html_document,
                                            self.max_amount_of_sentences, False,
                                            False)

    if embedding_result is None:
      self.logger.log_error(self.name, &#34;cant get embedding for &#34; + url)
      return error_result

    # create unitvector
    embedding = unit_vector(embedding_result[&#34;text_vector&#34;])

    distances = {}
    relative_distances = {}
    relevant = False
    guessed_category = &#34;not_relevant&#34;
    smallest_distance = 99999

    # calculate distance and relative distance for every category
    for category, ground_truth in self.ground_truth_vectors.items():
      distance = angle_between(embedding, ground_truth[&#34;embedding&#34;])
      distances[category] = distance
      relative_distances[category] = distance / ground_truth[&#34;allowed_distance&#34;]
      # document is relevant, now check to which cat it has the lowest relative distance
      if distance &lt;= ground_truth[&#34;allowed_distance&#34;]:
        relevant = True
        if relative_distances[category] &lt; smallest_distance:
          smallest_distance = relative_distances[category]
          guessed_category = category

    result = {
        &#34;relevant&#34;: relevant,
        &#34;distances&#34;: distances,
        &#34;relative_distances&#34;: relative_distances,
        &#34;guessed_category&#34;: guessed_category
    }

    return result

  def calculate_ideal_amount_of_sentences(
      self, dataset: dict, ignore_categories: bool) -&gt; (int, dict):
    &#34;&#34;&#34;Calculates the ideal amount of sentences per document for classifiying
      them

    Args:
      dataset: the dataset as dictionary
      ignore_categories: decides if categories are used or only one vector is
                          created

    Returns:
      the calculated amount of sentences that should be used and the sentence
      gradients so they can be saved
    &#34;&#34;&#34;

    amount_documents = 0
    sentence_gradients = {}
    gradient_limit = 0.02
    counter = 1

    self.logger.log_info(self.name, &#34;calculating ideal amount of sentences&#34;)

    # get amount of all relevant documents
    for category, items in dataset.items():
      if category != &#34;not_relevant&#34;:
        amount_documents += len(items)

    # iterate through all relevant documents and retrieve the sentence gradients
    for category, items in dataset.items():
      if category == &#34;not_relevant&#34;:
        continue
      # if we ignore labels, every label other than not_relevant is relevant
      if ignore_categories:
        category = &#34;relevant&#34;

      sentence_gradients[category] = []
      # get embedding and gradients for each document with max. amount of
      # sentences
      for item in items:
        print_progress_bar(counter, amount_documents)
        embedding_result = self.get_text_vector(item[&#34;document&#34;], 0, True,
                                                False)
        sentence_gradients[category].append({
            &#34;url&#34;: item[&#34;url&#34;],
            &#34;sentence_gradients&#34;: embedding_result[&#34;sentence_gradients&#34;]
        })
        counter += 1

    # calculate the ideal amount of sentences by averaging the amount of
    # sentences where gradient &lt; gradient_limit
    indices_gradient_limit_reached = []
    for category, items in sentence_gradients.items():
      for item in items:
        # set searched index to last element
        index_gradient_limit_reached = len(item[&#34;sentence_gradients&#34;]) - 1
        # look for index where gradient gets below gradient limit
        for index, value in enumerate(item[&#34;sentence_gradients&#34;]):
          if value &lt;= gradient_limit:
            index_gradient_limit_reached = index
            break
        indices_gradient_limit_reached.append(index_gradient_limit_reached)

    # get ideal value by averaging the ideal amount of sentences of each
    # document
    ideal_amount_of_sentences = ceil(
        sum(indices_gradient_limit_reached) /
        len(indices_gradient_limit_reached))

    return ideal_amount_of_sentences, sentence_gradients

  def generate_ground_truth_vectors(
      self,
      dataset: dict,
      ignore_categories: bool = False,
      max_amount_of_sentences: int = 0,
      allowed_distance_average: bool = False,
      get_most_important_sentences: bool = False) -&gt; dict:
    &#34;&#34;&#34;Generates new ground truth vectors from a csv file
        category not_relevant will be ignored

    Args:
      dataset_filename: filename of the csv file
      ignore_categories: decides if categories are used or only one vector is
                          created
      max_amount_of_sentences: amount of sentences considered for
                                classification (0=all)
      allowed_distance_average: decides if allowed_distance is calculated as
        average distance of each datapoint from ground truth vector or as the
        maximum distance from all points (per category)
      get_most_important_sentences: additionally returns dict of most important
        sentence per document

    Returns:
      a dict containing the generated vectors and one containing the gradients
        for each step and optionally the most important sentences
    &#34;&#34;&#34;

    # generate embedding for every url (still grouped by category)
    ground_truth_vectors = {}
    single_embeddings = {}
    ground_truth_gradient = {}
    result = {}

    counter = 1
    amount_documents = 0

    # get amount of all relevant documents
    for category, items in dataset.items():
      if category != &#34;not_relevant&#34;:
        amount_documents += len(items)

    if get_most_important_sentences:
      most_important_sentences = {}

    # process all documents: generate embedding and add it to the category
    # embedding
    for category, items in dataset.items():
      if category == &#34;not_relevant&#34;:
        continue
      # if categories should be ignored, replace category with one big
      # one (ground_truth)
      if ignore_categories:
        category = &#34;ground_truth&#34;
      for item in items:
        print_progress_bar(counter, amount_documents)
        self.logger.log_debug(
            self.name, &#34;generating embedding &#34; + str(counter) + &#34;/&#34; +
            str(amount_documents) + &#34; (&#34; + item[&#34;url&#34;] + &#34;)&#34;)
        try:
          # get embedding
          embedding_result = self.get_text_vector(item[&#34;document&#34;],
                                                  max_amount_of_sentences,
                                                  False,
                                                  get_most_important_sentences)

          # add new vector to the existing one to generate category embedding
          if category in ground_truth_vectors:
            old_ground_truth = ground_truth_vectors[category]
            new_ground_truth = [
                sum(i)
                for i in zip(embedding_result[&#34;text_vector&#34;], old_ground_truth)
            ]
            ground_truth_vectors[category] = new_ground_truth

            # calculate the gradient to see how much the vector changed
            gradient = angle_between(unit_vector(old_ground_truth),
                                     unit_vector(new_ground_truth))
            if category in ground_truth_gradient:
              ground_truth_gradient[category].append(gradient)
            else:
              ground_truth_gradient[category] = [gradient]

          else:
            ground_truth_vectors[category] = embedding_result[&#34;text_vector&#34;]

          # create the unit vector and save the single document embedding to
          # calculate allowed_distance later
          embedding = unit_vector(embedding_result[&#34;text_vector&#34;])
          if category in single_embeddings:
            single_embeddings[category].append({
                &#34;url&#34;: item[&#34;url&#34;],
                &#34;embedding&#34;: embedding
            })
          else:
            single_embeddings[category] = [{
                &#34;url&#34;: item[&#34;url&#34;],
                &#34;embedding&#34;: embedding
            }]
          # add most important sentences to result if requested
          if get_most_important_sentences:
            self.logger.log_debug(self.name, &#34;adding most important sentences&#34;)
            if category in most_important_sentences:
              most_important_sentences[category].append({
                  &#34;url&#34;:
                      item[&#34;url&#34;],
                  &#34;most_important_sentence&#34;:
                      embedding_result[&#34;most_important_sentence&#34;]
              })
            else:
              most_important_sentences[category] = [{
                  &#34;url&#34;:
                      item[&#34;url&#34;],
                  &#34;most_important_sentence&#34;:
                      embedding_result[&#34;most_important_sentence&#34;]
              }]
        except Exception as e:
          self.logger.log_warning(
              self.name, &#34;problem with &#34; + item[&#34;url&#34;] + &#34;(&#34; + str(e) + &#34;)&#34;)

        counter += 1
    print(&#34;\n&#34;)

    if get_most_important_sentences:
      result[&#34;most_important_sentences&#34;] = most_important_sentences

    self.logger.log_debug(self.name, &#34;ground truth vectors generated&#34;)

    # normalize all vectors for easier processing
    for category, vector in ground_truth_vectors.items():
      ground_truth_vectors[category] = {
          &#34;embedding&#34;: unit_vector(vector).tolist()
      }

    self.logger.log_debug(self.name, &#34;ground truth vectors normalized&#34;)

    # calculate allowed distance for each url from the calculated ground truth
    # vectors per category
    allowed_distances = {}

    if allowed_distance_average:
      # allowed distance is calculated as average distance from ground truth
      for category, entries in single_embeddings.items():
        allowed_distances[category] = 0
        # add up all distances per category
        for entry in entries:
          distance = angle_between(entry[&#34;embedding&#34;],
                                   ground_truth_vectors[category][&#34;embedding&#34;])
          allowed_distances[category] += distance

        # devide by amount of documents
        if len(entries) &gt; 0:
          allowed_distances[category] = allowed_distances[category] / len(
              entries)
        else:
          allowed_distances[category] = 0
    else:
      # allowed distance is calculated as max distance from ground truth
      for category, entries in single_embeddings.items():
        allowed_distances[category] = 0
        for entry in entries:
          distance = angle_between(entry[&#34;embedding&#34;],
                                   ground_truth_vectors[category][&#34;embedding&#34;])
          # remember the greatest seen distance
          if distance &gt; allowed_distances[category]:
            allowed_distances[category] = distance

    # save in ground truth vectors
    for category, allowed_distance in allowed_distances.items():
      ground_truth_vectors[category][&#34;allowed_distance&#34;] = allowed_distance

    result[&#34;ground_truth_vectors&#34;] = ground_truth_vectors
    result[&#34;ground_truth_gradients&#34;] = ground_truth_gradient

    return result

  def classify_bulk(self, dataset: dict) -&gt; str:
    &#34;&#34;&#34;Classifies the given dataset and returns metrics

    Args:
      dataset: a dict of a dataset

    Returns:
      metrics

    &#34;&#34;&#34;
    result = {}
    self.logger.log_debug(self.name, &#34;starting to classify bulk&#34;)

    amount_documents = 0
    counter = 0

    for category, items in dataset.items():
      amount_documents += len(items)

    if amount_documents == 0:
      return {}

    # iterate through all items
    for category, items in dataset.items():
      for item in items:
        counter += 1
        self.logger.log_debug(self.name, &#34;classifiying &#34; + item[&#34;url&#34;])
        print_progress_bar(counter, amount_documents)

        # calculate distances for the document
        classification_result = self.is_relevant(item[&#34;url&#34;], item[&#34;document&#34;])

        if category in result:
          result[category].append({
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;classification_result&#34;: classification_result
          })
        else:
          result[category] = [{
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;classification_result&#34;: classification_result
          }]
    print(&#34;\n&#34;)
    return result</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.calculate_ideal_amount_of_sentences"><code class="name flex">
<span>def <span class="ident">calculate_ideal_amount_of_sentences</span></span>(<span>self, dataset: dict, ignore_categories: bool) ‑> (<class 'int'>, <class 'dict'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the ideal amount of sentences per document for classifiying
them</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>the dataset as dictionary</dd>
<dt><strong><code>ignore_categories</code></strong></dt>
<dd>decides if categories are used or only one vector is
created</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the calculated amount of sentences that should be used and the sentence
gradients so they can be saved</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_ideal_amount_of_sentences(
    self, dataset: dict, ignore_categories: bool) -&gt; (int, dict):
  &#34;&#34;&#34;Calculates the ideal amount of sentences per document for classifiying
    them

  Args:
    dataset: the dataset as dictionary
    ignore_categories: decides if categories are used or only one vector is
                        created

  Returns:
    the calculated amount of sentences that should be used and the sentence
    gradients so they can be saved
  &#34;&#34;&#34;

  amount_documents = 0
  sentence_gradients = {}
  gradient_limit = 0.02
  counter = 1

  self.logger.log_info(self.name, &#34;calculating ideal amount of sentences&#34;)

  # get amount of all relevant documents
  for category, items in dataset.items():
    if category != &#34;not_relevant&#34;:
      amount_documents += len(items)

  # iterate through all relevant documents and retrieve the sentence gradients
  for category, items in dataset.items():
    if category == &#34;not_relevant&#34;:
      continue
    # if we ignore labels, every label other than not_relevant is relevant
    if ignore_categories:
      category = &#34;relevant&#34;

    sentence_gradients[category] = []
    # get embedding and gradients for each document with max. amount of
    # sentences
    for item in items:
      print_progress_bar(counter, amount_documents)
      embedding_result = self.get_text_vector(item[&#34;document&#34;], 0, True,
                                              False)
      sentence_gradients[category].append({
          &#34;url&#34;: item[&#34;url&#34;],
          &#34;sentence_gradients&#34;: embedding_result[&#34;sentence_gradients&#34;]
      })
      counter += 1

  # calculate the ideal amount of sentences by averaging the amount of
  # sentences where gradient &lt; gradient_limit
  indices_gradient_limit_reached = []
  for category, items in sentence_gradients.items():
    for item in items:
      # set searched index to last element
      index_gradient_limit_reached = len(item[&#34;sentence_gradients&#34;]) - 1
      # look for index where gradient gets below gradient limit
      for index, value in enumerate(item[&#34;sentence_gradients&#34;]):
        if value &lt;= gradient_limit:
          index_gradient_limit_reached = index
          break
      indices_gradient_limit_reached.append(index_gradient_limit_reached)

  # get ideal value by averaging the ideal amount of sentences of each
  # document
  ideal_amount_of_sentences = ceil(
      sum(indices_gradient_limit_reached) /
      len(indices_gradient_limit_reached))

  return ideal_amount_of_sentences, sentence_gradients</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.classify_bulk"><code class="name flex">
<span>def <span class="ident">classify_bulk</span></span>(<span>self, dataset: dict) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Classifies the given dataset and returns metrics</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>a dict of a dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classify_bulk(self, dataset: dict) -&gt; str:
  &#34;&#34;&#34;Classifies the given dataset and returns metrics

  Args:
    dataset: a dict of a dataset

  Returns:
    metrics

  &#34;&#34;&#34;
  result = {}
  self.logger.log_debug(self.name, &#34;starting to classify bulk&#34;)

  amount_documents = 0
  counter = 0

  for category, items in dataset.items():
    amount_documents += len(items)

  if amount_documents == 0:
    return {}

  # iterate through all items
  for category, items in dataset.items():
    for item in items:
      counter += 1
      self.logger.log_debug(self.name, &#34;classifiying &#34; + item[&#34;url&#34;])
      print_progress_bar(counter, amount_documents)

      # calculate distances for the document
      classification_result = self.is_relevant(item[&#34;url&#34;], item[&#34;document&#34;])

      if category in result:
        result[category].append({
            &#34;url&#34;: item[&#34;url&#34;],
            &#34;classification_result&#34;: classification_result
        })
      else:
        result[category] = [{
            &#34;url&#34;: item[&#34;url&#34;],
            &#34;classification_result&#34;: classification_result
        }]
  print(&#34;\n&#34;)
  return result</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.create_token_vectors"><code class="name flex">
<span>def <span class="ident">create_token_vectors</span></span>(<span>self, tokens_tensor: <built-in method tensor of type object at 0x7fe46782f9a0>, segments_tensor: <built-in method tensor of type object at 0x7fe46782f9a0>) ‑> list[float]</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the token vectors for each token in a sentence</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tokens_tensor</code></strong></dt>
<dd>a tensor containing the tokens of a sentence</dd>
<dt><strong><code>segments_tensor</code></strong></dt>
<dd>a tensor containing the segment tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a list of vectors, one for each word of the input sentence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_token_vectors(self, tokens_tensor: torch.tensor,
                         segments_tensor: torch.tensor) -&gt; list[float]:
  &#34;&#34;&#34;Creates the token vectors for each token in a sentence

  Args:
    tokens_tensor: a tensor containing the tokens of a sentence
    segments_tensor: a tensor containing the segment tensor

  Returns:
    a list of vectors, one for each word of the input sentence
  &#34;&#34;&#34;
  # freeze model
  with torch.no_grad():
    outputs = self.model(tokens_tensor, segments_tensor)
    # first layer is just input layer -&gt; remove it
    hidden_states = outputs[2]  #[1:]

  # extract the embeddings by concatenating the last 4 layers
  token_embeddings = hidden_states[-1]
  token_embeddings = torch.cat([hidden_states[i] for i in [-1, -2, -3, -4]],
                               dim=-1)
  # converting torchtensors to lists
  token_vectors = [token_embed.tolist() for token_embed in token_embeddings]

  return token_vectors[0]</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.generate_ground_truth_vectors"><code class="name flex">
<span>def <span class="ident">generate_ground_truth_vectors</span></span>(<span>self, dataset: dict, ignore_categories: bool = False, max_amount_of_sentences: int = 0, allowed_distance_average: bool = False, get_most_important_sentences: bool = False) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Generates new ground truth vectors from a csv file
category not_relevant will be ignored</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_filename</code></strong></dt>
<dd>filename of the csv file</dd>
<dt><strong><code>ignore_categories</code></strong></dt>
<dd>decides if categories are used or only one vector is
created</dd>
<dt><strong><code>max_amount_of_sentences</code></strong></dt>
<dd>amount of sentences considered for
classification (0=all)</dd>
<dt><strong><code>allowed_distance_average</code></strong></dt>
<dd>decides if allowed_distance is calculated as
average distance of each datapoint from ground truth vector or as the
maximum distance from all points (per category)</dd>
<dt><strong><code>get_most_important_sentences</code></strong></dt>
<dd>additionally returns dict of most important
sentence per document</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a dict containing the generated vectors and one containing the gradients
for each step and optionally the most important sentences</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_ground_truth_vectors(
    self,
    dataset: dict,
    ignore_categories: bool = False,
    max_amount_of_sentences: int = 0,
    allowed_distance_average: bool = False,
    get_most_important_sentences: bool = False) -&gt; dict:
  &#34;&#34;&#34;Generates new ground truth vectors from a csv file
      category not_relevant will be ignored

  Args:
    dataset_filename: filename of the csv file
    ignore_categories: decides if categories are used or only one vector is
                        created
    max_amount_of_sentences: amount of sentences considered for
                              classification (0=all)
    allowed_distance_average: decides if allowed_distance is calculated as
      average distance of each datapoint from ground truth vector or as the
      maximum distance from all points (per category)
    get_most_important_sentences: additionally returns dict of most important
      sentence per document

  Returns:
    a dict containing the generated vectors and one containing the gradients
      for each step and optionally the most important sentences
  &#34;&#34;&#34;

  # generate embedding for every url (still grouped by category)
  ground_truth_vectors = {}
  single_embeddings = {}
  ground_truth_gradient = {}
  result = {}

  counter = 1
  amount_documents = 0

  # get amount of all relevant documents
  for category, items in dataset.items():
    if category != &#34;not_relevant&#34;:
      amount_documents += len(items)

  if get_most_important_sentences:
    most_important_sentences = {}

  # process all documents: generate embedding and add it to the category
  # embedding
  for category, items in dataset.items():
    if category == &#34;not_relevant&#34;:
      continue
    # if categories should be ignored, replace category with one big
    # one (ground_truth)
    if ignore_categories:
      category = &#34;ground_truth&#34;
    for item in items:
      print_progress_bar(counter, amount_documents)
      self.logger.log_debug(
          self.name, &#34;generating embedding &#34; + str(counter) + &#34;/&#34; +
          str(amount_documents) + &#34; (&#34; + item[&#34;url&#34;] + &#34;)&#34;)
      try:
        # get embedding
        embedding_result = self.get_text_vector(item[&#34;document&#34;],
                                                max_amount_of_sentences,
                                                False,
                                                get_most_important_sentences)

        # add new vector to the existing one to generate category embedding
        if category in ground_truth_vectors:
          old_ground_truth = ground_truth_vectors[category]
          new_ground_truth = [
              sum(i)
              for i in zip(embedding_result[&#34;text_vector&#34;], old_ground_truth)
          ]
          ground_truth_vectors[category] = new_ground_truth

          # calculate the gradient to see how much the vector changed
          gradient = angle_between(unit_vector(old_ground_truth),
                                   unit_vector(new_ground_truth))
          if category in ground_truth_gradient:
            ground_truth_gradient[category].append(gradient)
          else:
            ground_truth_gradient[category] = [gradient]

        else:
          ground_truth_vectors[category] = embedding_result[&#34;text_vector&#34;]

        # create the unit vector and save the single document embedding to
        # calculate allowed_distance later
        embedding = unit_vector(embedding_result[&#34;text_vector&#34;])
        if category in single_embeddings:
          single_embeddings[category].append({
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;embedding&#34;: embedding
          })
        else:
          single_embeddings[category] = [{
              &#34;url&#34;: item[&#34;url&#34;],
              &#34;embedding&#34;: embedding
          }]
        # add most important sentences to result if requested
        if get_most_important_sentences:
          self.logger.log_debug(self.name, &#34;adding most important sentences&#34;)
          if category in most_important_sentences:
            most_important_sentences[category].append({
                &#34;url&#34;:
                    item[&#34;url&#34;],
                &#34;most_important_sentence&#34;:
                    embedding_result[&#34;most_important_sentence&#34;]
            })
          else:
            most_important_sentences[category] = [{
                &#34;url&#34;:
                    item[&#34;url&#34;],
                &#34;most_important_sentence&#34;:
                    embedding_result[&#34;most_important_sentence&#34;]
            }]
      except Exception as e:
        self.logger.log_warning(
            self.name, &#34;problem with &#34; + item[&#34;url&#34;] + &#34;(&#34; + str(e) + &#34;)&#34;)

      counter += 1
  print(&#34;\n&#34;)

  if get_most_important_sentences:
    result[&#34;most_important_sentences&#34;] = most_important_sentences

  self.logger.log_debug(self.name, &#34;ground truth vectors generated&#34;)

  # normalize all vectors for easier processing
  for category, vector in ground_truth_vectors.items():
    ground_truth_vectors[category] = {
        &#34;embedding&#34;: unit_vector(vector).tolist()
    }

  self.logger.log_debug(self.name, &#34;ground truth vectors normalized&#34;)

  # calculate allowed distance for each url from the calculated ground truth
  # vectors per category
  allowed_distances = {}

  if allowed_distance_average:
    # allowed distance is calculated as average distance from ground truth
    for category, entries in single_embeddings.items():
      allowed_distances[category] = 0
      # add up all distances per category
      for entry in entries:
        distance = angle_between(entry[&#34;embedding&#34;],
                                 ground_truth_vectors[category][&#34;embedding&#34;])
        allowed_distances[category] += distance

      # devide by amount of documents
      if len(entries) &gt; 0:
        allowed_distances[category] = allowed_distances[category] / len(
            entries)
      else:
        allowed_distances[category] = 0
  else:
    # allowed distance is calculated as max distance from ground truth
    for category, entries in single_embeddings.items():
      allowed_distances[category] = 0
      for entry in entries:
        distance = angle_between(entry[&#34;embedding&#34;],
                                 ground_truth_vectors[category][&#34;embedding&#34;])
        # remember the greatest seen distance
        if distance &gt; allowed_distances[category]:
          allowed_distances[category] = distance

  # save in ground truth vectors
  for category, allowed_distance in allowed_distances.items():
    ground_truth_vectors[category][&#34;allowed_distance&#34;] = allowed_distance

  result[&#34;ground_truth_vectors&#34;] = ground_truth_vectors
  result[&#34;ground_truth_gradients&#34;] = ground_truth_gradient

  return result</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_sentence_vector"><code class="name flex">
<span>def <span class="ident">get_sentence_vector</span></span>(<span>self, sentence) ‑> list[float]</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a embedding vector for the input sentence</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sentence</code></strong></dt>
<dd>a string for which an embedding is needed</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a vector which is an embedding of the input sentence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sentence_vector(self, sentence) -&gt; list[float]:
  &#34;&#34;&#34;Creates a embedding vector for the input sentence

  Args:
    sentence: a string for which an embedding is needed

  Returns:
    a vector which is an embedding of the input sentence
  &#34;&#34;&#34;

  if ML_MODEL == &#34;all-mpnet-base-v2&#34;:
    return self.model.encode(sentence)

  tokens_tensor, segments_tensor = self.pre_process_sentence(sentence)
  token_vectors = self.create_token_vectors(tokens_tensor, segments_tensor)
  # create sentence vector from token vectors
  sentence_vector = []
  for index in range(len(token_vectors[0])):
    vector_element = 0
    for token_vector in token_vectors:
      vector_element += token_vector[index]
    sentence_vector.append(vector_element)

  return sentence_vector</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_text_vector"><code class="name flex">
<span>def <span class="ident">get_text_vector</span></span>(<span>self, html: str, max_sentences: int = 0, generate_sentence_gradients: bool = False, get_most_important_sentence: bool = False) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an embedding vector for a whole document</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>html</code></strong></dt>
<dd>the document for which an embedding is needed</dd>
<dt><strong><code>max_sentences</code></strong></dt>
<dd>amount of sentences that should be considered, 0 = all</dd>
<dt><strong><code>generate_sentence_gradients</code></strong></dt>
<dd>if True, a gradient of the document vector
after every sentence is calculated and returned</dd>
<dt><strong><code>get_most_important_sentence</code></strong></dt>
<dd>returns the most informative sentence
(sentence with least difference to overall embedding)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a dict with text_vector and the sentence_gradients list if requested</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_text_vector(self,
                    html: str,
                    max_sentences: int = 0,
                    generate_sentence_gradients: bool = False,
                    get_most_important_sentence: bool = False) -&gt; dict:
  &#34;&#34;&#34;Creates an embedding vector for a whole document

  Args:
    html: the document for which an embedding is needed
    max_sentences: amount of sentences that should be considered, 0 = all
    generate_sentence_gradients: if True, a gradient of the document vector
      after every sentence is calculated and returned
    get_most_important_sentence: returns the most informative sentence
      (sentence with least difference to overall embedding)

  Returns:
    a dict with text_vector and the sentence_gradients list if requested
  &#34;&#34;&#34;
  # use trafilatura to extract main content
  main_content = trafilatura.extract(html, config=self.myconfig)

  if main_content is None:
    self.logger.log_warning(
        self.name, &#34;Trafilatura was not able to extract the main content&#34;)
    return None

  # split the main content into single sentences by splitting at
  # newline or sentence ending signs
  uncleaned_sentences = split(r&#34;\n|!|\.|\?&#34;, main_content)

  sentences = []

  # remove empty sentences and sentences that only consist of single words
  for sentence in uncleaned_sentences:
    if sentence.strip() == &#34;&#34;:
      continue
    if len(sentence.strip()) &lt;= 1:
      continue
    sentences.append(sentence.strip())

  self.logger.log_debug(self.name,
                        &#34;split into &#34; + str(len(sentences)) + &#34; sentences&#34;)

  if len(sentences) &lt; 1:
    return None

  # cut away unwanted sentences if too long
  if len(sentences) &gt; max_sentences &gt; 0:
    sentences = sentences[:max_sentences + 1]
    self.logger.log_debug(
        self.name, &#34;only &#34; + str(max_sentences) + &#34; sentences are used&#34;)

  sentence_vectors = []
  # generate a vector for every sentence
  i = 0
  for sentence in sentences:
    i += 1
    self.logger.log_debug(
        self.name, &#34;embedding sentence &#34; + str(i) + &#34;/&#34; + str(len(sentences)))
    sentence_vectors.append(self.get_sentence_vector(sentence))

  # create a whole vector by adding all sentence vectors
  text_vector = []
  # create 0 vector with correct length
  text_vector = [0 for _ in sentence_vectors[0]]

  if generate_sentence_gradients:
    sentence_gradients = []
    first = True
    # add up all sentences and create gradient after each step
    for sentence_vector in sentence_vectors:
      if first:
        text_vector = sentence_vector
        first = False
      else:
        old_text_vector = text_vector
        text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
        gradient = angle_between(unit_vector(old_text_vector),
                                 unit_vector(text_vector))
        sentence_gradients.append(gradient)

    result = {
        &#34;text_vector&#34;: text_vector,
        &#34;sentence_gradients&#34;: sentence_gradients
    }
  else:
    # just add up all sentence vectors
    for sentence_vector in sentence_vectors:
      text_vector = [sum(i) for i in zip(text_vector, sentence_vector)]
    result = {&#34;text_vector&#34;: text_vector}

  if get_most_important_sentence:
    index_min = len(sentence_vectors) - 1
    min_diff = 99999
    for index, sentence_vector in enumerate(sentence_vectors):
      current_diff = angle_between(unit_vector(sentence_vector),
                                   unit_vector(result[&#34;text_vector&#34;]))
      if current_diff &lt; min_diff:
        min_diff = current_diff
        index_min = index
    result[&#34;most_important_sentence&#34;] = sentences[index_min]

  return result</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.is_relevant"><code class="name flex">
<span>def <span class="ident">is_relevant</span></span>(<span>self, url: str, html_document: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the differences of the input document and decides if it is
relevant or not</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>url of the html document</dd>
<dt><strong><code>html_document</code></strong></dt>
<dd>the html document to be classified</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a dict containing "relevant" (bool), distances (to each category vector),
relative distances (to each category vector) and guessed_category</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_relevant(self, url: str, html_document: str) -&gt; dict:
  &#34;&#34;&#34;Calculates the differences of the input document and decides if it is
      relevant or not

  Args:
    url: url of the html document
    html_document: the html document to be classified

  Returns:
    a dict containing &#34;relevant&#34; (bool), distances (to each category vector),
    relative distances (to each category vector) and guessed_category
  &#34;&#34;&#34;
  error_result = {
      &#34;relevant&#34;: False,
      &#34;distances&#34;: {},
      &#34;relative_distances&#34;: {},
      &#34;guessed_category&#34;: &#34;not_relevant&#34;
  }

  if not hasattr(self, &#34;ground_truth_vectors&#34;):
    self.logger.log_critical(self.name, &#34;Ground truth vectors not loaded&#34;)
    self.monitor.stop_everything(&#34;Ground truth vectors not loaded&#34;)
    return error_result
  if not hasattr(self, &#34;max_amount_of_sentences&#34;):
    self.logger.log_critical(self.name, &#34;Max amount of sentences not set&#34;)
    self.monitor.stop_everything(&#34;Max amount of sentences not set&#34;)
    return error_result

  if html_document == &#34;&#34; or url == &#34;&#34;:
    return error_result

  # get embedding
  embedding_result = self.get_text_vector(html_document,
                                          self.max_amount_of_sentences, False,
                                          False)

  if embedding_result is None:
    self.logger.log_error(self.name, &#34;cant get embedding for &#34; + url)
    return error_result

  # create unitvector
  embedding = unit_vector(embedding_result[&#34;text_vector&#34;])

  distances = {}
  relative_distances = {}
  relevant = False
  guessed_category = &#34;not_relevant&#34;
  smallest_distance = 99999

  # calculate distance and relative distance for every category
  for category, ground_truth in self.ground_truth_vectors.items():
    distance = angle_between(embedding, ground_truth[&#34;embedding&#34;])
    distances[category] = distance
    relative_distances[category] = distance / ground_truth[&#34;allowed_distance&#34;]
    # document is relevant, now check to which cat it has the lowest relative distance
    if distance &lt;= ground_truth[&#34;allowed_distance&#34;]:
      relevant = True
      if relative_distances[category] &lt; smallest_distance:
        smallest_distance = relative_distances[category]
        guessed_category = category

  result = {
      &#34;relevant&#34;: relevant,
      &#34;distances&#34;: distances,
      &#34;relative_distances&#34;: relative_distances,
      &#34;guessed_category&#34;: guessed_category
  }

  return result</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.load_parameters_from_file"><code class="name flex">
<span>def <span class="ident">load_parameters_from_file</span></span>(<span>self, filename: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the ground truth vectors and max_amount_of_sentences from the given
file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong></dt>
<dd>name of the file which holds the ground truth vectors</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_parameters_from_file(self, filename: str) -&gt; None:
  &#34;&#34;&#34;Loads the ground truth vectors and max_amount_of_sentences from the given
      file

  Args:
    filename: name of the file which holds the ground truth vectors

  Returns:
    None
  &#34;&#34;&#34;
  with open(filename, encoding=&#34;utf-8&#34;) as x:
    data = json.load(x)
  self.ground_truth_vectors = data[&#34;ground_truth_vectors&#34;]
  self.max_amount_of_sentences = data[&#34;parameters&#34;][&#34;max_amount_of_sentences&#34;]</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.pre_process_sentence"><code class="name flex">
<span>def <span class="ident">pre_process_sentence</span></span>(<span>self, sentence) ‑> tuple[torch._VariableFunctionsClass.tensor, torch._VariableFunctionsClass.tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocesses the given sentence to use it as input for a BERT classifier</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sentence</code></strong></dt>
<dd>string of a sentence to be prepared</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a tuple of tensors to be used as input for a BERT classifier</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_process_sentence(self, sentence) -&gt; tuple[torch.tensor, torch.tensor]:
  &#34;&#34;&#34;Preprocesses the given sentence to use it as input for a BERT classifier

  Args:
    sentence: string of a sentence to be prepared

  Returns:
    a tuple of tensors to be used as input for a BERT classifier
  &#34;&#34;&#34;
  # add start token and seperator token at the end
  marked_text = &#34;[CLS] &#34; + sentence + &#34; [SEP]&#34;
  # tokenize the text
  tokenized_text = self.tokenizer.tokenize(marked_text)

  # cut away everything over 512 tokens (while keeping last token)
  if len(tokenized_text) &gt; 512:
    tokenized_text = tokenized_text[0:511] + [tokenized_text[-1]]

  # transform into ids
  indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)
  # create segment ids, we only have 1 sentence -&gt; only one vector with ones
  segments_ids = [1] * len(indexed_tokens)

  self.logger.log_debug(self.name,
                        &#34;amount of tokens: &#34; + str(len(segments_ids)))

  # create torch tensors
  tokens_tensor = torch.tensor([indexed_tokens])
  segments_tensor = torch.tensor([segments_ids])

  return tokens_tensor, segments_tensor</code></pre>
</details>
</dd>
<dt id="cybersecurity-crawler.src.crawler_bot.classification.Classifier.set_parameters"><code class="name flex">
<span>def <span class="ident">set_parameters</span></span>(<span>self, ground_truth_vectors: dict, max_amount_of_sentences: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sets new ground truth vectors and the amount of sentences to be used for
classification</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ground_truth_vectors</code></strong></dt>
<dd>a dict containing the new ground truth vectors</dd>
<dt><strong><code>max_sentences</code></strong></dt>
<dd>defines the max amount of sentences</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_parameters(self, ground_truth_vectors: dict,
                   max_amount_of_sentences: int) -&gt; None:
  &#34;&#34;&#34;Sets new ground truth vectors and the amount of sentences to be used for
      classification

  Args:
    ground_truth_vectors: a dict containing the new ground truth vectors
    max_sentences: defines the max amount of sentences

  Returns:
    None

  &#34;&#34;&#34;
  self.ground_truth_vectors = ground_truth_vectors
  self.max_amount_of_sentences = max_amount_of_sentences</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cybersecurity-crawler.src.crawler_bot" href="index.html">cybersecurity-crawler.src.crawler_bot</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier">Classifier</a></code></h4>
<ul class="">
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.calculate_ideal_amount_of_sentences" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.calculate_ideal_amount_of_sentences">calculate_ideal_amount_of_sentences</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.classify_bulk" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.classify_bulk">classify_bulk</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.create_token_vectors" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.create_token_vectors">create_token_vectors</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.generate_ground_truth_vectors" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.generate_ground_truth_vectors">generate_ground_truth_vectors</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_sentence_vector" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_sentence_vector">get_sentence_vector</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_text_vector" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.get_text_vector">get_text_vector</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.is_relevant" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.is_relevant">is_relevant</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.load_parameters_from_file" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.load_parameters_from_file">load_parameters_from_file</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.pre_process_sentence" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.pre_process_sentence">pre_process_sentence</a></code></li>
<li><code><a title="cybersecurity-crawler.src.crawler_bot.classification.Classifier.set_parameters" href="#cybersecurity-crawler.src.crawler_bot.classification.Classifier.set_parameters">set_parameters</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>